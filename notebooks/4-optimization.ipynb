{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### Imports"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T00:40:39.399157Z",
     "start_time": "2024-07-04T00:40:35.104315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, AutoModelForSequenceClassification, TrainingArguments, Trainer, Trainer, TrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kiddstudio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### Load data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T00:40:44.145657Z",
     "start_time": "2024-07-04T00:40:39.400410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "test_dataset = pd.read_csv('../data/processed/new_ds_test_dataset.csv')\n",
    "train_dataset = pd.read_csv('../data/processed/new_ds_train_dataset.csv')\n",
    "validation_dataset = pd.read_csv('../data/validation_set.csv')\n",
    "\n",
    "# Ensure all documents are strings\n",
    "train_dataset['document'] = train_dataset['document'][:150].astype(str)\n",
    "train_dataset['summary'] = train_dataset['summary'][:150].astype(str)\n",
    "test_dataset['document'] = test_dataset['document'][:50].astype(str)\n",
    "test_dataset['summary'] = test_dataset['summary'][:50].astype(str)\n",
    "validation_dataset['document'] = validation_dataset['document'][:50].astype(str)\n",
    "validation_dataset['summary'] = validation_dataset['summary'][:50].astype(str)\n",
    "\n",
    "# Handle missing values (remove rows with missing documents or summaries)\n",
    "train_dataset.dropna(subset=['document', 'summary'], inplace=True)\n",
    "test_dataset.dropna(subset=['document', 'summary'], inplace=True)\n",
    "validation_dataset.dropna(subset=['document', 'summary'], inplace=True)\n",
    "\n",
    "# Convert the pandas DataFrames to Hugging Face Datasets\n",
    "train = Dataset.from_pandas(train_dataset)\n",
    "test = Dataset.from_pandas(test_dataset)\n",
    "validation = Dataset.from_pandas(validation_dataset)\n",
    "\n",
    "# Reconstruct both datasets into a Dataset Dict object\n",
    "new_ds = DatasetDict({\n",
    "    'train': train,\n",
    "    'test': test,\n",
    "    'validation': validation\n",
    "})"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Showing the data here because the due to resource and time constraints, the entire dataset was not used nor uploaded to Github"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T00:40:44.151635Z",
     "start_time": "2024-07-04T00:40:44.146598Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset['document'][:10]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    national archives yes its time again folks its...\n",
       "1    los angeles ap  in first interview since nba b...\n",
       "2    gaithersburg md ap  a small private jet crashe...\n",
       "3    tucker carlson exposes his own sexism twitter ...\n",
       "4    a man accused removing another mans testicle m...\n",
       "5    suicide hotlines provide free confidential sup...\n",
       "6    croatia swastika hosts apologise nazi pitch sy...\n",
       "7    warczone collection outsideruploaded warcs con...\n",
       "8    vantage energy operates natural gas drilling s...\n",
       "9    photo roy hsu in lowdown world dining dashing ...\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T00:40:44.157040Z",
     "start_time": "2024-07-04T00:40:44.153608Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset['summary'][:10]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    – The unemployment rate dropped to 8.2% last m...\n",
       "1    – Shelly Sterling plans \"eventually\" to divorc...\n",
       "2    – A twin-engine Embraer jet that the FAA descr...\n",
       "3    – Tucker Carlson is in deep doodoo with conser...\n",
       "4    – What are the three most horrifying words in ...\n",
       "5    – Calls to suicide hotlines have spiked dramat...\n",
       "6    – Public apologies making headlines this week ...\n",
       "7    – Education Secretary John King has a message ...\n",
       "8    – A massive leak of fracking fluid poured into...\n",
       "9    – Paul Gonzales' approach to dating is similar...\n",
       "Name: summary, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T00:40:44.161163Z",
     "start_time": "2024-07-04T00:40:44.157888Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset['document'][:10]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    gop eyes gains as voters in 11 states pick gov...\n",
       "1    update 4192001 read richard metzger how i marr...\n",
       "2    its golden states latest version great secessi...\n",
       "3    the seed crawl list every host wayback machine...\n",
       "4    after year liberals scored impressive highprof...\n",
       "5    if true building set for demolition could be m...\n",
       "6    a still image taken israeli defence forces idf...\n",
       "7    paris ap  the pompidou centre paris hopes disp...\n",
       "8    starting 1996 alexa internet donating crawl da...\n",
       "9    the wounded officer crystal almeida 26 the thi...\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T00:40:44.165541Z",
     "start_time": "2024-07-04T00:40:44.162170Z"
    }
   },
   "cell_type": "code",
   "source": "test_dataset['summary'][:10]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    – It's a race for the governor's mansion in 11...\n",
       "1    – It turns out Facebook is only guilty of abou...\n",
       "2    – Not a big fan of Southern California? Neithe...\n",
       "3    – Why did Microsoft buy Nokia's phone business...\n",
       "4    – The Supreme Court is facing a docket of high...\n",
       "5    – In 1783, after the British soldiers left New...\n",
       "6    – Israel launched a round of airstrikes on Gaz...\n",
       "7    – A Picasso painting that was found to have va...\n",
       "8    – A dispute over the freshness of Wendy’s frie...\n",
       "9    – A 27-year-old Dallas police officer died Wed...\n",
       "Name: summary, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T00:40:44.168084Z",
     "start_time": "2024-07-04T00:40:44.166329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# new_ds = load_dataset(\"multi_news\", split=\"train\").shuffle(seed=42).select(range(200))\n",
    "# new_ds = new_ds.train_test_split(test_size=0.2)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### Tokenzer and Preprocessing function"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T00:40:45.579619Z",
     "start_time": "2024-07-04T00:40:44.168815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = new_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "# Setup evaluation\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d44fbaa7789d4f7b91a4a0bca7dd5a50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0ec075970d946d8981a76735bbd5eb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e4a6fca7c674e56bd845390f7a9ea06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### Compute Metrics"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T00:40:47.483840Z",
     "start_time": "2024-07-04T00:40:45.581134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # decode preds and labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return result\n",
    "\n",
    "# Load pretrained model and evaluate model after each epoch\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### Model training "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T01:44:01.978317Z",
     "start_time": "2024-07-04T00:40:47.488173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    predict_with_generate=True\n",
    ")\n",
    "\n",
    "trainer =  Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 1:00:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.017151</td>\n",
       "      <td>0.373953</td>\n",
       "      <td>0.102003</td>\n",
       "      <td>0.197713</td>\n",
       "      <td>0.312409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.966034</td>\n",
       "      <td>0.370452</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.196722</td>\n",
       "      <td>0.313146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=3.237326431274414, metrics={'train_runtime': 3794.2938, 'train_samples_per_second': 0.079, 'train_steps_per_second': 0.005, 'total_flos': 464373625651200.0, 'train_loss': 3.237326431274414, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
